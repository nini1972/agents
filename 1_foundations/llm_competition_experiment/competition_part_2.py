import os
import json
from dotenv import load_dotenv
import openai
from openai import OpenAI
from anthropic import Anthropic
import csv
import time
import random


import import_ipynb
import competition_part_1
  # No .ipynb extension

# Now you can access variables and functions from the notebook
question = competition_part_1.question
original_answer = competition_part_1.improved_answer


# Load environment variables (API keys etc)
load_dotenv(override=True)
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')
deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
groq_api_key = os.getenv('GROQ_API_KEY')

# Prompt templates
BASELINE_PROMPT = """
You are participating in a contest between several LLMs.
In the previous round, your response was ranked {rank} out of {num_competitors}.
Here is the top-ranked response from the last round:
---
{best_response}
---
Please provide your improvement to the orginal_answer {original_answer} that was given on the following question: {question}.
"""

EMOTIONAL_PROMPT = """
You are participating in a contest between several LLMs.
In the previous round, your response was ranked {rank} out of {num_competitors}.
{emotion}
Here is the top-ranked response from the last round:
---
{best_response}
---
Please provide your improvement to the orginal_answer {original_answer} that was given on the following question: {question}.
"""

EXPLICIT_PROMPT = """
You are participating in a contest between several LLMs.
In the previous round, your response was ranked {rank} out of {num_competitors}.
Here is the top-ranked response from the last round:
---
{best_response}
---
*Explicit instruction*: Learn from the top response above and try to improve your answer to aim for rank 1.
Please provide your improvement to the orginal_answer {original_answer} that was given on the following question: {question}.
"""

def get_emotion(rank, num_competitors):
    if rank == 1:
        return "Congratulations! You must feel proud to be ranked first."
    elif rank == num_competitors:
        return "You were ranked last. That might be disappointing, but you can do better!"
    else:
        return f"You were ranked {rank}. There's room for improvement‚Äîkeep going!"

def build_prompt(situation, rank, num_competitors, best_response):
    if situation == "baseline":
        return BASELINE_PROMPT.format(rank=rank, num_competitors=num_competitors, best_response=best_response)
    elif situation == "emotional":
        return EMOTIONAL_PROMPT.format(rank=rank, num_competitors=num_competitors, best_response=best_response, emotion=get_emotion(rank, num_competitors))
    elif situation == "explicit":
        return EXPLICIT_PROMPT.format(rank=rank, num_competitors=num_competitors, best_response=best_response)
    else:
        raise ValueError("Invalid situation")



def call_llm(llm_config, prompt, max_retries=5):
    """Calls the LLM with retry mechanism for transient errors."""
    messages = [{"role": "user", "content": prompt}]
    
    for attempt in range(max_retries):
        try:
            if isinstance(llm_config['client'], OpenAI):
                response = llm_config['client'].chat.completions.create(
                    model=llm_config['name'],
                    messages=messages,
                    max_tokens=1024
                )
                return response.choices[0].message.content
            
            elif isinstance(llm_config['client'], Anthropic):
                response = llm_config['client'].messages.create(
                    model=llm_config['name'],
                    messages=messages,
                    max_tokens=1024
                )
                return response.content[0].text
            
            else:
                raise ValueError(f"Unknown client for {llm_config['name']}")

        except openai.InternalServerError as e:
            wait_time = 2 ** attempt + random.uniform(0, 1)  # Exponential backoff with jitter
            print(f"Retry {attempt + 1}/{max_retries} for {llm_config['name']} due to server error. Waiting {wait_time:.2f}s...")
            time.sleep(wait_time)

        except Exception as e:
            print(f"Error calling {llm_config['name']}: {e}")
            return f"ERROR: {e}"

    return "ERROR: Max retries exceeded"
def judge_responses(judge_client, judge_model, responses, original_question, max_retries=5):
    """Calls the judge model with retry mechanism."""
    together = ""
    for idx, (llm, resp) in enumerate(responses.items()):
        together += f"# Review from competitor {idx+1}\n\n{resp}\n\n"

    judge_prompt = f"""
    You are judging a competition between {len(responses)} competitors.
    Each model has been given an answer {original_answer} generated by an LLM on a spefic question:{question}

    Each model has been requested to give advice for improvemnt on the answer presented to them.

    Your job is to evaluate each response on their plusvalue to improve the original answer, and rank them in order of best to worst.
    Respond with JSON, and only JSON, with the following format:
    {{"results": ["best competitor number", "second best competitor number", ...]}}

    Here are the responses from each competitor:

    {together}

    Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.
    """

    messages = [{"role": "user", "content": judge_prompt}]

    for attempt in range(max_retries):
        try:
            response = judge_client.chat.completions.create(
                model=judge_model,
                messages=messages,
                max_tokens=512
            )
            return json.loads(response.choices[0].message.content)["results"]

        except openai.InternalServerError as e:
            wait_time = 2 ** attempt + random.uniform(0, 1)
            print(f"Retry {attempt + 1}/{max_retries} for judge model due to server error. Waiting {wait_time:.2f}s...")
            time.sleep(wait_time)

        except Exception as e:
            print(f"Error in judge_responses: {e}")
            return []

    return []
def main():
    # Settings
    situations = ['baseline', 'emotional', 'explicit']
    num_rounds = 1
    num_runs = 1

    llm_configs = [
        {'name': 'gpt-4o-mini', 'client': OpenAI()},
        {'name': 'claude-3-7-sonnet-latest', 'client': Anthropic()},
        {'name': 'gemini-2.0-flash', 'client': OpenAI(api_key=google_api_key, base_url="https://generativelanguage.googleapis.com/v1beta/openai/")},
        {'name': 'deepseek-chat', 'client': OpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com/v1")},
        {'name': 'llama-3.3-70b-versatile', 'client': OpenAI(api_key=groq_api_key, base_url="https://api.groq.com/openai/v1")},
        {'name': 'llama3.2', 'client': OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')}
    ]

    for situation in situations:
        for run in range(1, num_runs+1):
            print(f"\n--- Situation: {situation} | Run {run} ---")
            original_question = question
            best_response = original_question
            round_histories = []
            last_ranks = {llm['name']: 1 for llm in llm_configs}
            for rnd in range(1, num_rounds+1):
                print(f"\nRound {rnd}")
                responses = {}
                for idx, llm in enumerate(llm_configs):
                    prompt = build_prompt(
                        situation,
                        last_ranks[llm['name']],
                        len(llm_configs),
                        best_response
                    )
                    try:
                        resp = call_llm(llm, prompt)
                    except Exception as e:
                        resp = f"ERROR: {e}"
                    responses[llm['name']] = resp
                    print(f"{llm['name']} (rank {last_ranks[llm['name']]}): {resp[:100]}...")  # print first 100 chars
                judge_client = OpenAI()
                judge_model = "gpt-4o"
                ranks = judge_responses(judge_client, judge_model, responses, original_question)
                rank_map = {}
                for idx, rank_llm_index in enumerate(ranks):
                    llm_name = llm_configs[int(rank_llm_index)-1]['name']
                    rank_map[llm_name] = idx+1
                print("Ranking:", rank_map)
                best_llm = min(rank_map, key=rank_map.get)
                best_response = responses[best_llm]
                last_ranks = rank_map
                round_histories.append({
                    "round": rnd,
                    "responses": responses,
                    "ranks": rank_map,
                    "best_llm": best_llm,
                    "best_response": best_response
                })
            folder = f"llm_competition_experiment/{situation}/run_{run}"
            os.makedirs(folder, exist_ok=True)
            with open(os.path.join(folder, "results.json"), "w") as f:
                json.dump(round_histories, f, indent=2)
            print(f"\nFinal ranking: {round_histories[-1]['ranks']}")
            print(f"Final responses: {round_histories[-1]['responses']}")

            summary_path = os.path.join(folder, "summary.txt")
            with open(summary_path, "w",encoding="utf-8") as f:
               for round_data in round_histories:
                   f.write(f"Round {round_data['round']}:\n")
                   for name, rank in sorted(round_data['ranks'].items(), key=lambda x: x[1]):
                       f.write(f"  {rank}. {name}\n")
                   f.write(f"  üèÜ Best: {round_data['best_llm']}\n\n")


            csv_path = os.path.join(folder, "results.csv")
            with open(csv_path, "w", newline="",encoding="utf-8") as csvfile:
                  writer = csv.writer(csvfile)
                  writer.writerow(["Round", "LLM", "Rank", "Best_Response"])
                  for round_data in round_histories:
                      for llm_name, rank in round_data["ranks"].items():
                          is_best = "‚úÖ" if llm_name == round_data["best_llm"] else ""
                          writer.writerow([
                          round_data["round"],
                          llm_name,
                          rank,
                          is_best
            ])



if __name__ == "__main__":
    main()